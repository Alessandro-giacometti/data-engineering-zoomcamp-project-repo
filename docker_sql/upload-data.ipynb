{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from nyc dataset\n",
    "Since the dataset is no longer in csv format, I convert it from parquet format to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file downloaded, converted to CSV, and saved as C:/Users/Aless/Documents/GIT/data-engineering-zoomcamp-project-repo/DOCKER_SQL/ny_taxi_postgres_data/yellow_tripdata_2021-01.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download the Parquet file and save it locally\n",
    "parquet_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet'\n",
    "local_parquet_path = 'C:/Users/Aless/Documents/GIT/data-engineering-zoomcamp-project-repo/DOCKER_SQL/ny_taxi_postgres_data/yellow_tripdata_2021-01.parquet'\n",
    "\n",
    "response = requests.get(parquet_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "with open(local_parquet_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Step 2: Read the Parquet file into a DataFrame\n",
    "df_parquet = pd.read_parquet(local_parquet_path)\n",
    "\n",
    "# Step 3: Write the DataFrame to a CSV file\n",
    "csv_file_path = 'C:/Users/Aless/Documents/GIT/data-engineering-zoomcamp-project-repo/DOCKER_SQL/ny_taxi_postgres_data/yellow_tripdata_2021-01.csv'\n",
    "df_parquet.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Parquet file downloaded, converted to CSV, and saved as {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file_path, nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"tpep_pickup_datetime\" TEXT,\n",
      "  \"tpep_dropoff_datetime\" TEXT,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL,\n",
      "  \"airport_fee\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "create_table_yellow_taxi_data = pd.io.sql.get_schema(df, name='yellow_taxi_data')\n",
    "print(create_table_yellow_taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"tpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"tpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL,\n",
      "  \"airport_fee\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "create_table_yellow_taxi_data = pd.io.sql.get_schema(df, name='yellow_taxi_data')\n",
    "print(create_table_yellow_taxi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection to PostGres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x1e348be2cf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53), \n",
      "\tairport_fee FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv(csv_file_path, iterator=True, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.parsers.readers.TextFileReader at 0x1e348c0e710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_next=next(df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_next.tpep_pickup_datetime = pd.to_datetime(df_next.tpep_pickup_datetime)\n",
    "df_next.tpep_dropoff_datetime = pd.to_datetime(df_next.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_next.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 500 ms\n",
      "Wall time: 999 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_next.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk..., took 0.930 second\n",
      "inserted another chunk..., took 1.097 second\n",
      "inserted another chunk..., took 1.155 second\n",
      "inserted another chunk..., took 1.172 second\n",
      "inserted another chunk..., took 1.411 second\n",
      "inserted another chunk..., took 1.157 second\n",
      "inserted another chunk..., took 1.034 second\n",
      "inserted another chunk..., took 1.011 second\n",
      "inserted another chunk..., took 0.924 second\n",
      "inserted another chunk..., took 1.010 second\n",
      "inserted another chunk..., took 1.011 second\n",
      "inserted another chunk..., took 1.119 second\n",
      "inserted another chunk..., took 0.997 second\n",
      "inserted another chunk..., took 1.079 second\n",
      "inserted another chunk..., took 0.985 second\n",
      "inserted another chunk..., took 1.178 second\n",
      "inserted another chunk..., took 1.028 second\n",
      "inserted another chunk..., took 1.125 second\n",
      "inserted another chunk..., took 1.131 second\n",
      "inserted another chunk..., took 1.041 second\n",
      "inserted another chunk..., took 0.980 second\n",
      "inserted another chunk..., took 1.041 second\n",
      "inserted another chunk..., took 1.097 second\n",
      "inserted another chunk..., took 1.014 second\n",
      "inserted another chunk..., took 1.144 second\n",
      "inserted another chunk..., took 1.078 second\n",
      "inserted another chunk..., took 0.983 second\n",
      "inserted another chunk..., took 0.889 second\n",
      "inserted another chunk..., took 1.064 second\n",
      "inserted another chunk..., took 1.131 second\n",
      "inserted another chunk..., took 0.978 second\n",
      "inserted another chunk..., took 0.961 second\n",
      "inserted another chunk..., took 1.041 second\n",
      "inserted another chunk..., took 0.980 second\n",
      "inserted another chunk..., took 0.969 second\n",
      "inserted another chunk..., took 0.912 second\n",
      "inserted another chunk..., took 0.933 second\n",
      "inserted another chunk..., took 0.882 second\n",
      "inserted another chunk..., took 1.004 second\n",
      "inserted another chunk..., took 0.940 second\n",
      "inserted another chunk..., took 0.988 second\n",
      "inserted another chunk..., took 1.159 second\n",
      "inserted another chunk..., took 0.915 second\n",
      "inserted another chunk..., took 0.881 second\n",
      "inserted another chunk..., took 1.027 second\n",
      "inserted another chunk..., took 1.190 second\n",
      "inserted another chunk..., took 1.088 second\n",
      "inserted another chunk..., took 0.940 second\n",
      "inserted another chunk..., took 1.119 second\n",
      "inserted another chunk..., took 0.954 second\n",
      "inserted another chunk..., took 1.108 second\n",
      "inserted another chunk..., took 1.005 second\n",
      "inserted another chunk..., took 0.883 second\n",
      "inserted another chunk..., took 1.171 second\n",
      "inserted another chunk..., took 0.919 second\n",
      "inserted another chunk..., took 1.080 second\n",
      "inserted another chunk..., took 1.015 second\n",
      "inserted another chunk..., took 1.019 second\n",
      "inserted another chunk..., took 1.149 second\n",
      "inserted another chunk..., took 1.278 second\n",
      "inserted another chunk..., took 1.081 second\n",
      "inserted another chunk..., took 1.036 second\n",
      "inserted another chunk..., took 0.988 second\n",
      "inserted another chunk..., took 0.997 second\n",
      "inserted another chunk..., took 1.043 second\n",
      "inserted another chunk..., took 1.032 second\n",
      "inserted another chunk..., took 1.035 second\n",
      "inserted another chunk..., took 1.182 second\n",
      "inserted another chunk..., took 0.973 second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aless\\AppData\\Local\\Temp\\ipykernel_13260\\2460734351.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_next.tpep_dropoff_datetime = pd.to_datetime(df_next.tpep_dropoff_datetime)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk..., took 0.981 second\n",
      "inserted another chunk..., took 1.020 second\n",
      "inserted another chunk..., took 1.115 second\n",
      "inserted another chunk..., took 0.968 second\n",
      "inserted another chunk..., took 1.145 second\n",
      "inserted another chunk..., took 0.980 second\n",
      "inserted another chunk..., took 0.945 second\n",
      "inserted another chunk..., took 1.029 second\n",
      "inserted another chunk..., took 0.982 second\n",
      "inserted another chunk..., took 0.972 second\n",
      "inserted another chunk..., took 1.110 second\n",
      "inserted another chunk..., took 0.988 second\n",
      "inserted another chunk..., took 0.923 second\n",
      "inserted another chunk..., took 1.064 second\n",
      "inserted another chunk..., took 1.007 second\n",
      "inserted another chunk..., took 1.035 second\n",
      "inserted another chunk..., took 0.933 second\n",
      "inserted another chunk..., took 0.953 second\n",
      "inserted another chunk..., took 1.088 second\n",
      "inserted another chunk..., took 1.024 second\n",
      "inserted another chunk..., took 0.899 second\n",
      "inserted another chunk..., took 0.843 second\n",
      "inserted another chunk..., took 1.075 second\n",
      "inserted another chunk..., took 1.056 second\n",
      "inserted another chunk..., took 0.930 second\n",
      "inserted another chunk..., took 0.989 second\n",
      "inserted another chunk..., took 1.101 second\n",
      "inserted another chunk..., took 1.044 second\n",
      "inserted another chunk..., took 0.885 second\n",
      "inserted another chunk..., took 1.067 second\n",
      "inserted another chunk..., took 0.903 second\n",
      "inserted another chunk..., took 0.920 second\n",
      "inserted another chunk..., took 0.964 second\n",
      "inserted another chunk..., took 0.876 second\n",
      "inserted another chunk..., took 0.942 second\n",
      "inserted another chunk..., took 0.978 second\n",
      "inserted another chunk..., took 0.929 second\n",
      "inserted another chunk..., took 0.918 second\n",
      "inserted another chunk..., took 1.092 second\n",
      "inserted another chunk..., took 0.911 second\n",
      "inserted another chunk..., took 1.031 second\n",
      "inserted another chunk..., took 0.919 second\n",
      "inserted another chunk..., took 0.887 second\n",
      "inserted another chunk..., took 1.151 second\n",
      "inserted another chunk..., took 0.887 second\n",
      "inserted another chunk..., took 1.070 second\n",
      "inserted another chunk..., took 1.024 second\n",
      "inserted another chunk..., took 1.156 second\n",
      "inserted another chunk..., took 1.027 second\n",
      "inserted another chunk..., took 1.058 second\n",
      "inserted another chunk..., took 1.078 second\n",
      "inserted another chunk..., took 1.016 second\n",
      "inserted another chunk..., took 1.024 second\n",
      "inserted another chunk..., took 1.009 second\n",
      "inserted another chunk..., took 1.194 second\n",
      "inserted another chunk..., took 1.210 second\n",
      "inserted another chunk..., took 1.047 second\n",
      "inserted another chunk..., took 1.131 second\n",
      "inserted another chunk..., took 0.896 second\n",
      "inserted another chunk..., took 1.158 second\n",
      "inserted another chunk..., took 1.057 second\n",
      "inserted another chunk..., took 0.958 second\n",
      "inserted another chunk..., took 1.113 second\n",
      "inserted another chunk..., took 1.229 second\n",
      "inserted another chunk..., took 1.053 second\n",
      "inserted another chunk..., took 0.915 second\n",
      "inserted another chunk..., took 0.916 second\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     t_start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m----> 3\u001b[0m     df_next\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     df_next\u001b[38;5;241m.\u001b[39mtpep_pickup_datetime \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_next\u001b[38;5;241m.\u001b[39mtpep_pickup_datetime)\n\u001b[0;32m      6\u001b[0m     df_next\u001b[38;5;241m.\u001b[39mtpep_dropoff_datetime \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_next\u001b[38;5;241m.\u001b[39mtpep_dropoff_datetime)\n",
      "File \u001b[1;32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:863\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    t_start = time()\n",
    "    df_next=next(df_iter)\n",
    "\n",
    "    df_next.tpep_pickup_datetime = pd.to_datetime(df_next.tpep_pickup_datetime)\n",
    "    df_next.tpep_dropoff_datetime = pd.to_datetime(df_next.tpep_dropoff_datetime)\n",
    "\n",
    "    df_next.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    "\n",
    "    t_end = time()\n",
    "\n",
    "    print('inserted another chunk..., took %.3f second' % (t_end - t_start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
